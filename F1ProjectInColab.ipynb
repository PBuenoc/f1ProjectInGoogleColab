{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "pU2ZlM2mDLm9",
        "pqQ0lLJ4DhHc",
        "GBEY6uMFEmT1"
      ],
      "authorship_tag": "ABX9TyMaGEqHTmPtrqa/RbigevZc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PBuenoc/f1ProjectInGoogleColab/blob/main/F1ProjectInColab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ingestion"
      ],
      "metadata": {
        "id": "er-D0hJLN3MP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare spark environment"
      ],
      "metadata": {
        "id": "pU2ZlM2mDLm9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-3.1.2/spark-3.1.2-bin-hadoop2.7.tgz\n",
        "!tar xf spark-3.1.2-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark"
      ],
      "metadata": {
        "id": "Gw3cNTCrg1dN"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.2-bin-hadoop2.7\""
      ],
      "metadata": {
        "id": "ANLqmU37Lbbw"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import findspark\n",
        "findspark.init()"
      ],
      "metadata": {
        "id": "1WR8q-sBL-nP"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master('local[*]').appName(\"Iniciando com Spark\").getOrCreate()"
      ],
      "metadata": {
        "id": "NEpuIzesMP5X"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ingest csv files"
      ],
      "metadata": {
        "id": "NSE1ZPIjXAKu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ingest circuits.csv file"
      ],
      "metadata": {
        "id": "pqQ0lLJ4DhHc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "circuits_df = spark.read.csv('/content/formula1/data/raw/circuits2.csv',inferSchema=True, header=True, sep=';')"
      ],
      "metadata": {
        "id": "nFEAMzx0M2lY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Select only the columns required"
      ],
      "metadata": {
        "id": "1PS5_CoBD2Q0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "circuits_df = circuits_df.select('circuitId', 'circuitRef', 'name', 'location', 'country', 'lat', 'lng', 'alt')"
      ],
      "metadata": {
        "id": "xuxEzkvjr4SG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Renamed the columns as required"
      ],
      "metadata": {
        "id": "jSIcGi-9EBIl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "circuits_df = circuits_df.withColumnRenamed('circuitId', 'circuit_id') \\\n",
        ".withColumnRenamed('circuitRef', 'circuit_ref') \\\n",
        ".withColumnRenamed('lat','latitude') \\\n",
        ".withColumnRenamed('lng','longitude') \\\n",
        ".withColumnRenamed('alt','altitude')"
      ],
      "metadata": {
        "id": "B7Wki6VTpGxd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Add ingestion_date column"
      ],
      "metadata": {
        "id": "gwLS7o4CEFSE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import current_timestamp"
      ],
      "metadata": {
        "id": "_246Ktopu5vH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "circuits_df = circuits_df.withColumn('ingestion_date', current_timestamp())"
      ],
      "metadata": {
        "id": "kPowGTxWvGJX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Specify the types as required"
      ],
      "metadata": {
        "id": "o0grVm4wELhM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "circuits_df = circuits_df.withColumn('latitude', circuits_df['latitude'].cast('double')) \\\n",
        ".withColumn('longitude', circuits_df['longitude'].cast('double')) \\\n",
        ".withColumn('altitude', circuits_df['altitude'].cast('integer'))"
      ],
      "metadata": {
        "id": "lDS9NPySwWnA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Write the data in parquet format on processed folder"
      ],
      "metadata": {
        "id": "2aE6W-WrEPcN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "circuits_df.write.mode('overwrite').parquet('/content/formula1/data/processed/circuits')"
      ],
      "metadata": {
        "id": "p82B_aXZxP8w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ingest races.csv"
      ],
      "metadata": {
        "id": "GBEY6uMFEmT1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "races_df = spark.read.csv('/content/formula1/data/raw/races2.csv', header=True, inferSchema=True, sep=';')"
      ],
      "metadata": {
        "id": "ECm8bxcF22M1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Add the required columns"
      ],
      "metadata": {
        "id": "3rwT5-vvEqIl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, concat, lit"
      ],
      "metadata": {
        "id": "ud7mgeJI9NB4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "races_df = races_df.withColumn('ingestion_date', current_timestamp()) \\\n",
        ".withColumn('race_timestamp',concat(col('date'), lit(' '), col('time')))"
      ],
      "metadata": {
        "id": "hMjY_hA28iAA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Select only the required columns"
      ],
      "metadata": {
        "id": "Kath6f4HExGf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "races_df = races_df.select(col('raceId').alias('race_id'),\n",
        "                           col('year').alias('race_year'), \n",
        "                           col('round'), \n",
        "                           col('circuitId').alias('circuit_id'),\n",
        "                           col('name'),\n",
        "                           col('ingestion_date'),\n",
        "                           col('race_timestamp'))"
      ],
      "metadata": {
        "id": "DoFFb-3O9igY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Write the data in parquet format on processed folder with partitionBy"
      ],
      "metadata": {
        "id": "rb2vc9UUE4MF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "races_df.write.mode('overwrite').partitionBy('race_year').parquet('/content/formula1/data/processed/races')"
      ],
      "metadata": {
        "id": "o5iJaaKZAFwZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ingest JSON files"
      ],
      "metadata": {
        "id": "2KISlZ_wXKHm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ingest constructors.json"
      ],
      "metadata": {
        "id": "Io3qSwQPFGpF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "constructors_schema = \"constructorId INT, constructorRef STRING, name STRING, nationality STRING, url STRING\""
      ],
      "metadata": {
        "id": "znhIjqZxAyUK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "constructors_df = spark.read.json('/content/formula1/data/raw/constructors.json', schema=constructors_schema)"
      ],
      "metadata": {
        "id": "VI6EffEcJe9I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Drop unwanted columns from the dataframe"
      ],
      "metadata": {
        "id": "pz2VO1AmUns1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "constructors_df = constructors_df.drop(constructors_df.url)"
      ],
      "metadata": {
        "id": "mwpBOQRqPkuh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Rename columns and add ingestion date"
      ],
      "metadata": {
        "id": "InVZnAnbVGaN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "constructors_df = constructors_df.withColumnRenamed('constructorId', 'constructor_id') \\\n",
        "                                .withColumnRenamed('constructorRef', 'constructor_ref') \\\n",
        "                                .withColumn('ingestion_date', current_timestamp())"
      ],
      "metadata": {
        "id": "UWsWWaHWVCoN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "constructors_df.write.mode('overwrite').parquet('/content/formula1/data/processed/constructors')"
      ],
      "metadata": {
        "id": "sqQY6JI8VwO1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ingest drivers.json - Nested JSON"
      ],
      "metadata": {
        "id": "Wo5I62IXXlZm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DateType"
      ],
      "metadata": {
        "id": "w-ikKCV20nEr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "name_schema = StructType(fields=[StructField(\"forename\", StringType(), True),\n",
        "                                 StructField(\"surname\", StringType(), True)\n",
        "  \n",
        "])"
      ],
      "metadata": {
        "id": "Pdtm98202UAl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drivers_schema = StructType(fields=[StructField(\"driverId\", IntegerType(), False),\n",
        "                                    StructField(\"driverRef\", StringType(), True),\n",
        "                                    StructField(\"number\", IntegerType(), True),\n",
        "                                    StructField(\"code\", StringType(), True),\n",
        "                                    StructField(\"name\", name_schema),\n",
        "                                    StructField(\"dob\", DateType(), True),\n",
        "                                    StructField(\"nationality\", StringType(), True),\n",
        "                                    StructField(\"url\", StringType(), True)])"
      ],
      "metadata": {
        "id": "3sMG6t6Z2Wok"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drivers_df = spark.read \\\n",
        ".schema(drivers_schema) \\\n",
        ".json('/content/formula1/data/raw/drivers.json')"
      ],
      "metadata": {
        "id": "8iimmyy12Y5N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Rename and add new columns"
      ],
      "metadata": {
        "id": "b6cjfBtS4Tyk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, concat, lit"
      ],
      "metadata": {
        "id": "unk4qAyZ62IX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drivers_df = drivers_df.withColumnRenamed(\"driverId\", \"driver_id\") \\\n",
        "                                    .withColumnRenamed(\"driverRef\", \"driver_ref\") \\\n",
        "                                    .withColumn(\"name\", concat(col(\"name.forename\"), lit(\" \"), col(\"name.surname\")))"
      ],
      "metadata": {
        "id": "8Ou9pltF3y_1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import current_timestamp"
      ],
      "metadata": {
        "id": "7RHGFBTW5Frf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drivers_df = drivers_df.withColumn('ingestion_date', current_timestamp())"
      ],
      "metadata": {
        "id": "rfD9HKDy684H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Drop unwanted columns\n",
        "url \\\n",
        "name.forename \\\n",
        "name.surname \\"
      ],
      "metadata": {
        "id": "tDymPf-_776e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "drivers_df = drivers_df.drop('url')"
      ],
      "metadata": {
        "id": "EnKYPEih7Sdv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Write the output to processed folder in parquet format\n"
      ],
      "metadata": {
        "id": "Sq57Q8XZ87Ir"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "drivers_df.write.mode('overwrite').parquet('/content/formula1/data/processed/drivers')"
      ],
      "metadata": {
        "id": "aGAQBAmN8vSA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ingest results.json"
      ],
      "metadata": {
        "id": "AcE9lSS4-r-h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, FloatType"
      ],
      "metadata": {
        "id": "0NO3zl3m9jog"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results_schema = StructType(fields=[StructField(\"resultId\", IntegerType(), False),\n",
        "                                    StructField(\"raceId\", IntegerType(), True),\n",
        "                                    StructField(\"driverId\", IntegerType(), True),\n",
        "                                    StructField(\"constructorId\", IntegerType(), True),\n",
        "                                    StructField(\"number\", IntegerType(), True),\n",
        "                                    StructField(\"grid\", IntegerType(), True),\n",
        "                                    StructField(\"position\", IntegerType(), True),\n",
        "                                    StructField(\"positionText\", StringType(), True),\n",
        "                                    StructField(\"positionOrder\", IntegerType(), True),\n",
        "                                    StructField(\"points\", FloatType(), True),\n",
        "                                    StructField(\"laps\", IntegerType(), True),\n",
        "                                    StructField(\"time\", StringType(), True),\n",
        "                                    StructField(\"milliseconds\", IntegerType(), True),\n",
        "                                    StructField(\"fastestLap\", IntegerType(), True),\n",
        "                                    StructField(\"rank\", IntegerType(), True),\n",
        "                                    StructField(\"fastestLapTime\", StringType(), True),\n",
        "                                    StructField(\"fastestLapSpeed\", FloatType(), True),\n",
        "                                    StructField(\"statusId\", StringType(), True)])"
      ],
      "metadata": {
        "id": "9SNwL9v7DaO7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results_df = spark.read \\\n",
        ".schema(results_schema) \\\n",
        ".json('/content/formula1/data/raw/results.json')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qhvZ10ewCWjb",
        "outputId": "49088d93-4a10-420b-9db3-88851d66bc1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+------+--------+-------------+------+----+--------+------------+-------------+------+----+-----------+------------+----------+----+--------------+---------------+--------+\n",
            "|resultId|raceId|driverId|constructorId|number|grid|position|positionText|positionOrder|points|laps|       time|milliseconds|fastestLap|rank|fastestLapTime|fastestLapSpeed|statusId|\n",
            "+--------+------+--------+-------------+------+----+--------+------------+-------------+------+----+-----------+------------+----------+----+--------------+---------------+--------+\n",
            "|       1|    18|       1|            1|    22|   1|       1|           1|            1|  10.0|  58|1:34:50.616|     5690616|        39|   2|      1:27.452|          218.3|       1|\n",
            "|       2|    18|       2|            2|     3|   5|       2|           2|            2|   8.0|  58|     +5.478|     5696094|        41|   3|      1:27.739|        217.586|       1|\n",
            "|       3|    18|       3|            3|     7|   7|       3|           3|            3|   6.0|  58|     +8.163|     5698779|        41|   5|      1:28.090|        216.719|       1|\n",
            "|       4|    18|       4|            4|     5|  11|       4|           4|            4|   5.0|  58|    +17.181|     5707797|        58|   7|      1:28.603|        215.464|       1|\n",
            "|       5|    18|       5|            1|    23|   3|       5|           5|            5|   4.0|  58|    +18.014|     5708630|        43|   1|      1:27.418|        218.385|       1|\n",
            "|       6|    18|       6|            3|     8|  13|       6|           6|            6|   3.0|  57|         \\N|        null|        50|  14|      1:29.639|        212.974|      11|\n",
            "|       7|    18|       7|            5|    14|  17|       7|           7|            7|   2.0|  55|         \\N|        null|        22|  12|      1:29.534|        213.224|       5|\n",
            "|       8|    18|       8|            6|     1|  15|       8|           8|            8|   1.0|  53|         \\N|        null|        20|   4|      1:27.903|         217.18|       5|\n",
            "|       9|    18|       9|            2|     4|   2|    null|           R|            9|   0.0|  47|         \\N|        null|        15|   9|      1:28.753|          215.1|       4|\n",
            "|      10|    18|      10|            7|    12|  18|    null|           R|           10|   0.0|  43|         \\N|        null|        23|  13|      1:29.558|        213.166|       3|\n",
            "|      11|    18|      11|            8|    18|  19|    null|           R|           11|   0.0|  32|         \\N|        null|        24|  15|      1:30.892|        210.038|       7|\n",
            "|      12|    18|      12|            4|     6|  20|    null|           R|           12|   0.0|  30|         \\N|        null|        20|  16|      1:31.384|        208.907|       8|\n",
            "|      13|    18|      13|            6|     2|   4|    null|           R|           13|   0.0|  29|         \\N|        null|        23|   6|      1:28.175|         216.51|       5|\n",
            "|      14|    18|      14|            9|     9|   8|    null|           R|           14|   0.0|  25|         \\N|        null|        21|  11|      1:29.502|          213.3|       4|\n",
            "|      15|    18|      15|            7|    11|   6|    null|           R|           15|   0.0|  19|         \\N|        null|        18|  10|      1:29.310|        213.758|      10|\n",
            "|      16|    18|      16|           10|    20|  22|    null|           R|           16|   0.0|   8|         \\N|        null|         8|  17|      1:32.021|        207.461|       9|\n",
            "|      17|    18|      17|            9|    10|  14|    null|           R|           17|   0.0|   0|         \\N|        null|      null|null|            \\N|           null|       4|\n",
            "|      18|    18|      18|           11|    16|  12|    null|           R|           18|   0.0|   0|         \\N|        null|      null|null|            \\N|           null|       4|\n",
            "|      19|    18|      19|            8|    19|  21|    null|           R|           19|   0.0|   0|         \\N|        null|      null|null|            \\N|           null|       4|\n",
            "|      20|    18|      20|            5|    15|   9|    null|           R|           20|   0.0|   0|         \\N|        null|      null|null|            \\N|           null|       4|\n",
            "+--------+------+--------+-------------+------+----+--------+------------+-------------+------+----+-----------+------------+----------+----+--------------+---------------+--------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Drop, rename and add required columns"
      ],
      "metadata": {
        "id": "nxvFwGNUE4bb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results_df = results_df.drop('statusId')"
      ],
      "metadata": {
        "id": "6LtTeMHoCxJT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results_df = results_df.withColumnRenamed('resultId', 'result_id') \\\n",
        "                       .withColumnRenamed('raceId', 'race_id') \\\n",
        "                       .withColumnRenamed('driverId', 'driver_id') \\\n",
        "                       .withColumnRenamed('constructorId', 'constructor_id') \\\n",
        "                       .withColumnRenamed('positionText', 'position_text') \\\n",
        "                       .withColumnRenamed('positionOrder', 'position_order') \\\n",
        "                       .withColumnRenamed('fastestLap', 'fastest_lap') \\\n",
        "                       .withColumnRenamed('fastestLapTime', 'fastest_lap_time') \\\n",
        "                       .withColumnRenamed('fastestLapSpeed', 'fastest_lap_speed')"
      ],
      "metadata": {
        "id": "E4HyTUY7EFOE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import current_timestamp"
      ],
      "metadata": {
        "id": "GpjnqAGfFWMs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results_df = results_df.withColumn('ingestion_date', current_timestamp())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v658u2deE3vj",
        "outputId": "08db9548-c201-4548-cd7b-6669fd7e445f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+-------+---------+--------------+------+----+--------+-------------+--------------+------+----+-----------+------------+-----------+----+----------------+-----------------+--------------------+\n",
            "|result_id|race_id|driver_id|constructor_id|number|grid|position|position_text|position_order|points|laps|       time|milliseconds|fastest_lap|rank|fastest_lap_time|fastest_lap_speed|      ingestion_date|\n",
            "+---------+-------+---------+--------------+------+----+--------+-------------+--------------+------+----+-----------+------------+-----------+----+----------------+-----------------+--------------------+\n",
            "|        1|     18|        1|             1|    22|   1|       1|            1|             1|  10.0|  58|1:34:50.616|     5690616|         39|   2|        1:27.452|            218.3|2023-03-16 01:43:...|\n",
            "|        2|     18|        2|             2|     3|   5|       2|            2|             2|   8.0|  58|     +5.478|     5696094|         41|   3|        1:27.739|          217.586|2023-03-16 01:43:...|\n",
            "|        3|     18|        3|             3|     7|   7|       3|            3|             3|   6.0|  58|     +8.163|     5698779|         41|   5|        1:28.090|          216.719|2023-03-16 01:43:...|\n",
            "|        4|     18|        4|             4|     5|  11|       4|            4|             4|   5.0|  58|    +17.181|     5707797|         58|   7|        1:28.603|          215.464|2023-03-16 01:43:...|\n",
            "|        5|     18|        5|             1|    23|   3|       5|            5|             5|   4.0|  58|    +18.014|     5708630|         43|   1|        1:27.418|          218.385|2023-03-16 01:43:...|\n",
            "|        6|     18|        6|             3|     8|  13|       6|            6|             6|   3.0|  57|         \\N|        null|         50|  14|        1:29.639|          212.974|2023-03-16 01:43:...|\n",
            "|        7|     18|        7|             5|    14|  17|       7|            7|             7|   2.0|  55|         \\N|        null|         22|  12|        1:29.534|          213.224|2023-03-16 01:43:...|\n",
            "|        8|     18|        8|             6|     1|  15|       8|            8|             8|   1.0|  53|         \\N|        null|         20|   4|        1:27.903|           217.18|2023-03-16 01:43:...|\n",
            "|        9|     18|        9|             2|     4|   2|    null|            R|             9|   0.0|  47|         \\N|        null|         15|   9|        1:28.753|            215.1|2023-03-16 01:43:...|\n",
            "|       10|     18|       10|             7|    12|  18|    null|            R|            10|   0.0|  43|         \\N|        null|         23|  13|        1:29.558|          213.166|2023-03-16 01:43:...|\n",
            "|       11|     18|       11|             8|    18|  19|    null|            R|            11|   0.0|  32|         \\N|        null|         24|  15|        1:30.892|          210.038|2023-03-16 01:43:...|\n",
            "|       12|     18|       12|             4|     6|  20|    null|            R|            12|   0.0|  30|         \\N|        null|         20|  16|        1:31.384|          208.907|2023-03-16 01:43:...|\n",
            "|       13|     18|       13|             6|     2|   4|    null|            R|            13|   0.0|  29|         \\N|        null|         23|   6|        1:28.175|           216.51|2023-03-16 01:43:...|\n",
            "|       14|     18|       14|             9|     9|   8|    null|            R|            14|   0.0|  25|         \\N|        null|         21|  11|        1:29.502|            213.3|2023-03-16 01:43:...|\n",
            "|       15|     18|       15|             7|    11|   6|    null|            R|            15|   0.0|  19|         \\N|        null|         18|  10|        1:29.310|          213.758|2023-03-16 01:43:...|\n",
            "|       16|     18|       16|            10|    20|  22|    null|            R|            16|   0.0|   8|         \\N|        null|          8|  17|        1:32.021|          207.461|2023-03-16 01:43:...|\n",
            "|       17|     18|       17|             9|    10|  14|    null|            R|            17|   0.0|   0|         \\N|        null|       null|null|              \\N|             null|2023-03-16 01:43:...|\n",
            "|       18|     18|       18|            11|    16|  12|    null|            R|            18|   0.0|   0|         \\N|        null|       null|null|              \\N|             null|2023-03-16 01:43:...|\n",
            "|       19|     18|       19|             8|    19|  21|    null|            R|            19|   0.0|   0|         \\N|        null|       null|null|              \\N|             null|2023-03-16 01:43:...|\n",
            "|       20|     18|       20|             5|    15|   9|    null|            R|            20|   0.0|   0|         \\N|        null|       null|null|              \\N|             null|2023-03-16 01:43:...|\n",
            "+---------+-------+---------+--------------+------+----+--------+-------------+--------------+------+----+-----------+------------+-----------+----+----------------+-----------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Write the output to processed folder in parquet format"
      ],
      "metadata": {
        "id": "4tlZGkmlFnwE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results_df.write.mode('overwrite').partitionBy('race_id').parquet('/content/formula1/data/processed/results')\n"
      ],
      "metadata": {
        "id": "9tb8375tFFVU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ingest pitstops.json"
      ],
      "metadata": {
        "id": "DJm7rFGPGwTk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import StructType, StructField, IntegerType, StringType"
      ],
      "metadata": {
        "id": "BgQbNkGmGKG0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pit_stops_schema = StructType(fields=[StructField(\"raceId\", IntegerType(), False),\n",
        "                                      StructField(\"driverId\", IntegerType(), True),\n",
        "                                      StructField(\"stop\", StringType(), True),\n",
        "                                      StructField(\"lap\", IntegerType(), True),\n",
        "                                      StructField(\"time\", StringType(), True),\n",
        "                                      StructField(\"duration\", StringType(), True),\n",
        "                                      StructField(\"milliseconds\", IntegerType(), True)\n",
        "                                     ])"
      ],
      "metadata": {
        "id": "tE8SMmWtHUYm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pit_stops_df = spark.read \\\n",
        ".schema(pit_stops_schema) \\\n",
        ".option('multiline', True) \\\n",
        ".json('/content/formula1/data/raw/pit_stops.json')"
      ],
      "metadata": {
        "id": "TlRglmlKHWeG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Rename and add columns as required"
      ],
      "metadata": {
        "id": "9KOrVMQsH3Fm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pit_stops_df = pit_stops_df.withColumnRenamed('raceId', 'race_id') \\\n",
        "                           .withColumnRenamed('driverId', 'driver_id')"
      ],
      "metadata": {
        "id": "eQJ4O5vFHp8V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pit_stops_df = pit_stops_df.withColumn('ingestion_date', current_timestamp())"
      ],
      "metadata": {
        "id": "Mn_ybx9wINNG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Write the output to processed folder in parquet format"
      ],
      "metadata": {
        "id": "6UG1V5B8IlU9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pit_stops_df.write.mode(\"overwrite\").parquet('/content/formula1/data/processed/pit_stops')"
      ],
      "metadata": {
        "id": "ngFa_eYhIg0G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ingest multiple files"
      ],
      "metadata": {
        "id": "VVX-eE-JR4fT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ingest lap_times folder"
      ],
      "metadata": {
        "id": "9nMYHhLRSBcq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, FloatType"
      ],
      "metadata": {
        "id": "Uv6ZlQNbSA3q"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lap_times_schema = StructType(fields=[StructField('raceId', IntegerType(), False),\n",
        "                                      StructField('driverId', IntegerType(), False),\n",
        "                                      StructField('lap', IntegerType(), False),\n",
        "                                      StructField('position', IntegerType(), True),\n",
        "                                      StructField('time', StringType(), True),\n",
        "                                      StructField('milliseconds', IntegerType(), True),\n",
        "])"
      ],
      "metadata": {
        "id": "TE3G1G6TSvNi"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lap_times_df = spark.read \\\n",
        ".schema(lap_times_schema) \\\n",
        ".csv('/content/formula1/data/raw/lap_times')"
      ],
      "metadata": {
        "id": "ku1cm6-cT9-p"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import current_timestamp"
      ],
      "metadata": {
        "id": "DDBbKWFKUz2Z"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lap_times_df = lap_times_df.withColumnRenamed('raceId', 'race_id') \\\n",
        "                           .withColumnRenamed('driverId', 'driver_id') \\\n",
        "                           .withColumn('ingestion_date', current_timestamp())"
      ],
      "metadata": {
        "id": "xlvPpysfUO6h"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lap_times_df.write.mode('overwrite').parquet('/content/formula1/data/processed/lap_times')"
      ],
      "metadata": {
        "id": "kcaFOQwTUUFZ"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ingest qualifying files - Multi files of MultIline JSON "
      ],
      "metadata": {
        "id": "5OI7RVGoINKi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "qualifying_schema = StructType(fields=[StructField('qualifyId', IntegerType(), False),\n",
        "                                         StructField('raceId', IntegerType(), True),\n",
        "                                         StructField('driverId', IntegerType(), True),\n",
        "                                         StructField('constructorId', IntegerType(), True),\n",
        "                                         StructField('number', IntegerType(), True),\n",
        "                                         StructField('position', IntegerType(), True),\n",
        "                                         StructField('q1', StringType(), True),\n",
        "                                         StructField('q2', StringType(), True),\n",
        "                                         StructField('q3', StringType(), True),\n",
        "  \n",
        "])"
      ],
      "metadata": {
        "id": "Fcn_tyko_CTM"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qualifying_df = spark.read \\\n",
        ".schema(qualifying_schema) \\\n",
        ".option('multiLine', True) \\\n",
        ".json('/content/formula1/data/raw/qualifying/*')"
      ],
      "metadata": {
        "id": "77GeDC94JbLr"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qualifying_df = qualifying_df.withColumnRenamed('qualifyId', 'qualify_id') \\\n",
        "                             .withColumnRenamed('raceId', 'race_id') \\\n",
        "                             .withColumnRenamed('driverId', 'driver_id') \\\n",
        "                             .withColumnRenamed('constructorId', 'constructor_id') \\\n",
        "                             .withColumn('ingestion_date', current_timestamp())"
      ],
      "metadata": {
        "id": "wrlneaFZJ2VK"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qualifying_df.write.mode('overwrite').parquet('/content/formula1/data/processed/qualifying')"
      ],
      "metadata": {
        "id": "YScc7wBsJ4ZZ"
      },
      "execution_count": 62,
      "outputs": []
    }
  ]
}