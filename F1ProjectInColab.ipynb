{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "pU2ZlM2mDLm9",
        "pqQ0lLJ4DhHc",
        "GBEY6uMFEmT1"
      ],
      "authorship_tag": "ABX9TyMaGEqHTmPtrqa/RbigevZc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PBuenoc/f1ProjectInGoogleColab/blob/main/F1ProjectInColab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ingestion"
      ],
      "metadata": {
        "id": "er-D0hJLN3MP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare spark environment"
      ],
      "metadata": {
        "id": "pU2ZlM2mDLm9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-3.1.2/spark-3.1.2-bin-hadoop2.7.tgz\n",
        "!tar xf spark-3.1.2-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark"
      ],
      "metadata": {
        "id": "Gw3cNTCrg1dN"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.2-bin-hadoop2.7\""
      ],
      "metadata": {
        "id": "ANLqmU37Lbbw"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import findspark\n",
        "findspark.init()"
      ],
      "metadata": {
        "id": "1WR8q-sBL-nP"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master('local[*]').appName(\"Iniciando com Spark\").getOrCreate()"
      ],
      "metadata": {
        "id": "NEpuIzesMP5X"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ingest csv files"
      ],
      "metadata": {
        "id": "NSE1ZPIjXAKu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ingest circuits.csv file"
      ],
      "metadata": {
        "id": "pqQ0lLJ4DhHc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "circuits_df = spark.read.csv('/content/formula1/data/raw/circuits2.csv',inferSchema=True, header=True, sep=';')"
      ],
      "metadata": {
        "id": "nFEAMzx0M2lY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Select only the columns required"
      ],
      "metadata": {
        "id": "1PS5_CoBD2Q0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "circuits_df = circuits_df.select('circuitId', 'circuitRef', 'name', 'location', 'country', 'lat', 'lng', 'alt')"
      ],
      "metadata": {
        "id": "xuxEzkvjr4SG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Renamed the columns as required"
      ],
      "metadata": {
        "id": "jSIcGi-9EBIl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "circuits_df = circuits_df.withColumnRenamed('circuitId', 'circuit_id') \\\n",
        ".withColumnRenamed('circuitRef', 'circuit_ref') \\\n",
        ".withColumnRenamed('lat','latitude') \\\n",
        ".withColumnRenamed('lng','longitude') \\\n",
        ".withColumnRenamed('alt','altitude')"
      ],
      "metadata": {
        "id": "B7Wki6VTpGxd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Add ingestion_date column"
      ],
      "metadata": {
        "id": "gwLS7o4CEFSE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import current_timestamp"
      ],
      "metadata": {
        "id": "_246Ktopu5vH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "circuits_df = circuits_df.withColumn('ingestion_date', current_timestamp())"
      ],
      "metadata": {
        "id": "kPowGTxWvGJX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Specify the types as required"
      ],
      "metadata": {
        "id": "o0grVm4wELhM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "circuits_df = circuits_df.withColumn('latitude', circuits_df['latitude'].cast('double')) \\\n",
        ".withColumn('longitude', circuits_df['longitude'].cast('double')) \\\n",
        ".withColumn('altitude', circuits_df['altitude'].cast('integer'))"
      ],
      "metadata": {
        "id": "lDS9NPySwWnA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Write the data in parquet format on processed folder"
      ],
      "metadata": {
        "id": "2aE6W-WrEPcN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "circuits_df.write.mode('overwrite').parquet('/content/formula1/data/processed/circuits')"
      ],
      "metadata": {
        "id": "p82B_aXZxP8w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ingest races.csv"
      ],
      "metadata": {
        "id": "GBEY6uMFEmT1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "races_df = spark.read.csv('/content/formula1/data/raw/races2.csv', header=True, inferSchema=True, sep=';')"
      ],
      "metadata": {
        "id": "ECm8bxcF22M1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Add the required columns"
      ],
      "metadata": {
        "id": "3rwT5-vvEqIl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, concat, lit"
      ],
      "metadata": {
        "id": "ud7mgeJI9NB4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "races_df = races_df.withColumn('ingestion_date', current_timestamp()) \\\n",
        ".withColumn('race_timestamp',concat(col('date'), lit(' '), col('time')))"
      ],
      "metadata": {
        "id": "hMjY_hA28iAA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Select only the required columns"
      ],
      "metadata": {
        "id": "Kath6f4HExGf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "races_df = races_df.select(col('raceId').alias('race_id'),\n",
        "                           col('year').alias('race_year'), \n",
        "                           col('round'), \n",
        "                           col('circuitId').alias('circuit_id'),\n",
        "                           col('name'),\n",
        "                           col('ingestion_date'),\n",
        "                           col('race_timestamp'))"
      ],
      "metadata": {
        "id": "DoFFb-3O9igY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Write the data in parquet format on processed folder with partitionBy"
      ],
      "metadata": {
        "id": "rb2vc9UUE4MF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "races_df.write.mode('overwrite').partitionBy('race_year').parquet('/content/formula1/data/processed/races')"
      ],
      "metadata": {
        "id": "o5iJaaKZAFwZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ingest JSON files"
      ],
      "metadata": {
        "id": "2KISlZ_wXKHm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ingest constructors.json"
      ],
      "metadata": {
        "id": "Io3qSwQPFGpF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "constructors_schema = \"constructorId INT, constructorRef STRING, name STRING, nationality STRING, url STRING\""
      ],
      "metadata": {
        "id": "znhIjqZxAyUK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "constructors_df = spark.read.json('/content/formula1/data/raw/constructors.json', schema=constructors_schema)"
      ],
      "metadata": {
        "id": "VI6EffEcJe9I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Drop unwanted columns from the dataframe"
      ],
      "metadata": {
        "id": "pz2VO1AmUns1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "constructors_df = constructors_df.drop(constructors_df.url)"
      ],
      "metadata": {
        "id": "mwpBOQRqPkuh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Rename columns and add ingestion date"
      ],
      "metadata": {
        "id": "InVZnAnbVGaN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "constructors_df = constructors_df.withColumnRenamed('constructorId', 'constructor_id') \\\n",
        "                                .withColumnRenamed('constructorRef', 'constructor_ref') \\\n",
        "                                .withColumn('ingestion_date', current_timestamp())"
      ],
      "metadata": {
        "id": "UWsWWaHWVCoN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "constructors_df.write.mode('overwrite').parquet('/content/formula1/data/processed/constructors')"
      ],
      "metadata": {
        "id": "sqQY6JI8VwO1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ingest drivers.json - Nested JSON"
      ],
      "metadata": {
        "id": "Wo5I62IXXlZm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DateType"
      ],
      "metadata": {
        "id": "w-ikKCV20nEr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "name_schema = StructType(fields=[StructField(\"forename\", StringType(), True),\n",
        "                                 StructField(\"surname\", StringType(), True)\n",
        "  \n",
        "])"
      ],
      "metadata": {
        "id": "Pdtm98202UAl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drivers_schema = StructType(fields=[StructField(\"driverId\", IntegerType(), False),\n",
        "                                    StructField(\"driverRef\", StringType(), True),\n",
        "                                    StructField(\"number\", IntegerType(), True),\n",
        "                                    StructField(\"code\", StringType(), True),\n",
        "                                    StructField(\"name\", name_schema),\n",
        "                                    StructField(\"dob\", DateType(), True),\n",
        "                                    StructField(\"nationality\", StringType(), True),\n",
        "                                    StructField(\"url\", StringType(), True)])"
      ],
      "metadata": {
        "id": "3sMG6t6Z2Wok"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drivers_df = spark.read \\\n",
        ".schema(drivers_schema) \\\n",
        ".json('/content/formula1/data/raw/drivers.json')"
      ],
      "metadata": {
        "id": "8iimmyy12Y5N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Rename and add new columns"
      ],
      "metadata": {
        "id": "b6cjfBtS4Tyk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, concat, lit"
      ],
      "metadata": {
        "id": "unk4qAyZ62IX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drivers_df = drivers_df.withColumnRenamed(\"driverId\", \"driver_id\") \\\n",
        "                                    .withColumnRenamed(\"driverRef\", \"driver_ref\") \\\n",
        "                                    .withColumn(\"name\", concat(col(\"name.forename\"), lit(\" \"), col(\"name.surname\")))"
      ],
      "metadata": {
        "id": "8Ou9pltF3y_1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import current_timestamp"
      ],
      "metadata": {
        "id": "7RHGFBTW5Frf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drivers_df = drivers_df.withColumn('ingestion_date', current_timestamp())"
      ],
      "metadata": {
        "id": "rfD9HKDy684H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Drop unwanted columns\n",
        "url \\\n",
        "name.forename \\\n",
        "name.surname \\"
      ],
      "metadata": {
        "id": "tDymPf-_776e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "drivers_df = drivers_df.drop('url')"
      ],
      "metadata": {
        "id": "EnKYPEih7Sdv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Write the output to processed folder in parquet format\n"
      ],
      "metadata": {
        "id": "Sq57Q8XZ87Ir"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "drivers_df.write.mode('overwrite').parquet('/content/formula1/data/processed/drivers')"
      ],
      "metadata": {
        "id": "aGAQBAmN8vSA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ingest results.json"
      ],
      "metadata": {
        "id": "AcE9lSS4-r-h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, FloatType"
      ],
      "metadata": {
        "id": "0NO3zl3m9jog"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results_schema = StructType(fields=[StructField(\"resultId\", IntegerType(), False),\n",
        "                                    StructField(\"raceId\", IntegerType(), True),\n",
        "                                    StructField(\"driverId\", IntegerType(), True),\n",
        "                                    StructField(\"constructorId\", IntegerType(), True),\n",
        "                                    StructField(\"number\", IntegerType(), True),\n",
        "                                    StructField(\"grid\", IntegerType(), True),\n",
        "                                    StructField(\"position\", IntegerType(), True),\n",
        "                                    StructField(\"positionText\", StringType(), True),\n",
        "                                    StructField(\"positionOrder\", IntegerType(), True),\n",
        "                                    StructField(\"points\", FloatType(), True),\n",
        "                                    StructField(\"laps\", IntegerType(), True),\n",
        "                                    StructField(\"time\", StringType(), True),\n",
        "                                    StructField(\"milliseconds\", IntegerType(), True),\n",
        "                                    StructField(\"fastestLap\", IntegerType(), True),\n",
        "                                    StructField(\"rank\", IntegerType(), True),\n",
        "                                    StructField(\"fastestLapTime\", StringType(), True),\n",
        "                                    StructField(\"fastestLapSpeed\", FloatType(), True),\n",
        "                                    StructField(\"statusId\", StringType(), True)])"
      ],
      "metadata": {
        "id": "9SNwL9v7DaO7"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results_df = spark.read \\\n",
        ".schema(results_schema) \\\n",
        ".json('/content/formula1/data/raw/results.json')"
      ],
      "metadata": {
        "id": "qhvZ10ewCWjb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Drop, rename and add required columns"
      ],
      "metadata": {
        "id": "nxvFwGNUE4bb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results_df = results_df.drop('statusId')"
      ],
      "metadata": {
        "id": "6LtTeMHoCxJT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results_df = results_df.withColumnRenamed('resultId', 'result_id') \\\n",
        "                       .withColumnRenamed('raceId', 'race_id') \\\n",
        "                       .withColumnRenamed('driverId', 'driver_id') \\\n",
        "                       .withColumnRenamed('constructorId', 'constructor_id') \\\n",
        "                       .withColumnRenamed('positionText', 'position_text') \\\n",
        "                       .withColumnRenamed('positionOrder', 'position_order') \\\n",
        "                       .withColumnRenamed('fastestLap', 'fastest_lap') \\\n",
        "                       .withColumnRenamed('fastestLapTime', 'fastest_lap_time') \\\n",
        "                       .withColumnRenamed('fastestLapSpeed', 'fastest_lap_speed')"
      ],
      "metadata": {
        "id": "E4HyTUY7EFOE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import current_timestamp"
      ],
      "metadata": {
        "id": "GpjnqAGfFWMs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results_df = results_df.withColumn('ingestion_date', current_timestamp())"
      ],
      "metadata": {
        "id": "v658u2deE3vj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Write the output to processed folder in parquet format"
      ],
      "metadata": {
        "id": "4tlZGkmlFnwE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results_df.write.mode('overwrite').partitionBy('race_id').parquet('/content/formula1/data/processed/results')\n"
      ],
      "metadata": {
        "id": "9tb8375tFFVU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ingest pitstops.json"
      ],
      "metadata": {
        "id": "DJm7rFGPGwTk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import StructType, StructField, IntegerType, StringType"
      ],
      "metadata": {
        "id": "BgQbNkGmGKG0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pit_stops_schema = StructType(fields=[StructField(\"raceId\", IntegerType(), False),\n",
        "                                      StructField(\"driverId\", IntegerType(), True),\n",
        "                                      StructField(\"stop\", StringType(), True),\n",
        "                                      StructField(\"lap\", IntegerType(), True),\n",
        "                                      StructField(\"time\", StringType(), True),\n",
        "                                      StructField(\"duration\", StringType(), True),\n",
        "                                      StructField(\"milliseconds\", IntegerType(), True)\n",
        "                                     ])"
      ],
      "metadata": {
        "id": "tE8SMmWtHUYm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pit_stops_df = spark.read \\\n",
        ".schema(pit_stops_schema) \\\n",
        ".option('multiline', True) \\\n",
        ".json('/content/formula1/data/raw/pit_stops.json')"
      ],
      "metadata": {
        "id": "TlRglmlKHWeG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Rename and add columns as required"
      ],
      "metadata": {
        "id": "9KOrVMQsH3Fm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pit_stops_df = pit_stops_df.withColumnRenamed('raceId', 'race_id') \\\n",
        "                           .withColumnRenamed('driverId', 'driver_id')"
      ],
      "metadata": {
        "id": "eQJ4O5vFHp8V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pit_stops_df = pit_stops_df.withColumn('ingestion_date', current_timestamp())"
      ],
      "metadata": {
        "id": "Mn_ybx9wINNG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Write the output to processed folder in parquet format"
      ],
      "metadata": {
        "id": "6UG1V5B8IlU9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pit_stops_df.write.mode(\"overwrite\").parquet('/content/formula1/data/processed/pit_stops')"
      ],
      "metadata": {
        "id": "ngFa_eYhIg0G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ingest multiple files"
      ],
      "metadata": {
        "id": "VVX-eE-JR4fT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ingest lap_times folder"
      ],
      "metadata": {
        "id": "9nMYHhLRSBcq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, FloatType"
      ],
      "metadata": {
        "id": "Uv6ZlQNbSA3q"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lap_times_schema = StructType(fields=[StructField('raceId', IntegerType(), False),\n",
        "                                      StructField('driverId', IntegerType(), False),\n",
        "                                      StructField('lap', IntegerType(), False),\n",
        "                                      StructField('position', IntegerType(), True),\n",
        "                                      StructField('time', StringType(), True),\n",
        "                                      StructField('milliseconds', IntegerType(), True),\n",
        "])"
      ],
      "metadata": {
        "id": "TE3G1G6TSvNi"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lap_times_df = spark.read \\\n",
        ".schema(lap_times_schema) \\\n",
        ".csv('/content/formula1/data/raw/lap_times')"
      ],
      "metadata": {
        "id": "ku1cm6-cT9-p"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import current_timestamp"
      ],
      "metadata": {
        "id": "DDBbKWFKUz2Z"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lap_times_df = lap_times_df.withColumnRenamed('raceId', 'race_id') \\\n",
        "                           .withColumnRenamed('driverId', 'driver_id') \\\n",
        "                           .withColumn('ingestion_date', current_timestamp())"
      ],
      "metadata": {
        "id": "xlvPpysfUO6h"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lap_times_df.write.mode('overwrite').parquet('/content/formula1/data/processed/lap_times')"
      ],
      "metadata": {
        "id": "kcaFOQwTUUFZ"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ingest qualifying files - Multi files of MultIline JSON "
      ],
      "metadata": {
        "id": "5OI7RVGoINKi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "qualifying_schema = StructType(fields=[StructField('qualifyId', IntegerType(), False),\n",
        "                                         StructField('raceId', IntegerType(), True),\n",
        "                                         StructField('driverId', IntegerType(), True),\n",
        "                                         StructField('constructorId', IntegerType(), True),\n",
        "                                         StructField('number', IntegerType(), True),\n",
        "                                         StructField('position', IntegerType(), True),\n",
        "                                         StructField('q1', StringType(), True),\n",
        "                                         StructField('q2', StringType(), True),\n",
        "                                         StructField('q3', StringType(), True),\n",
        "  \n",
        "])"
      ],
      "metadata": {
        "id": "Fcn_tyko_CTM"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qualifying_df = spark.read \\\n",
        ".schema(qualifying_schema) \\\n",
        ".option('multiLine', True) \\\n",
        ".json('/content/formula1/data/raw/qualifying/*')"
      ],
      "metadata": {
        "id": "77GeDC94JbLr"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qualifying_df = qualifying_df.withColumnRenamed('qualifyId', 'qualify_id') \\\n",
        "                             .withColumnRenamed('raceId', 'race_id') \\\n",
        "                             .withColumnRenamed('driverId', 'driver_id') \\\n",
        "                             .withColumnRenamed('constructorId', 'constructor_id') \\\n",
        "                             .withColumn('ingestion_date', current_timestamp())"
      ],
      "metadata": {
        "id": "wrlneaFZJ2VK"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qualifying_df.write.mode('overwrite').parquet('/content/formula1/data/processed/qualifying')"
      ],
      "metadata": {
        "id": "YScc7wBsJ4ZZ"
      },
      "execution_count": 62,
      "outputs": []
    }
  ]
}